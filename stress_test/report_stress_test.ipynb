{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"Avenir Next Condensed\"><b><u>REPORT of STRESS TESTS</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><font face=\"Avenir Next Condensed\"><b><u>Detailing of the hardware used to run the test:</b>\n",
    ">>Proccesor:\tIntel Core i5 dual core\n",
    "  <br>Processor speed: 2,7 GHz</br>\n",
    "  Memorie:\t8 GB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Avenir Next Condensed\"><b><u><h4>Stress Test Analysis</h4></b></u>\n",
    "\n",
    "<b><u>1. Details of the tests</b></u>\n",
    "\n",
    "<font face=\"Avenir Next Condensed\"><b><i>Total ratio</b></i>\n",
    "\n",
    "100.0% Users\n",
    "<br>28.6% Index microservices\n",
    "<br>71.4% Predict microservices\n",
    "\n",
    "\n",
    "<font face=\"Avenir Next Condensed\"><b><i>More importance ('or weight') to the predict microservice to test Â¿Why?</b></i>\n",
    "\n",
    "The stress test was realized with more weight on the predict. The reason is that the  microservices of predict requires more use of memorie and process. And in the other hand it's possible what the users get into the web and stay using the predict microservices upload different type of photos to predict, for more time. At the same time new users continue get into the web and using the same microservice. \n",
    "So that, the fluency of users trought the 'index' to the 'predict' it would be faster. But, these fluency it might not happend on the 'predict' microservices.\n",
    "\n",
    "<font face=\"Avenir Next Condensed\"><b><i>How the tests were realized?</b></i>\n",
    "\n",
    "<i> Each tests was realized during the same time (15 minutes) and with the same image. </i>\n",
    "- In the first stage four tests were tested:\n",
    "    - Test 1: 20 users concurrents and 3 users per second. \n",
    "    - Test 2: 40 users concurrents and 6 users per second.\n",
    "    - Test 3: 80 users concurrents and 12 users per second.\n",
    "    - Test 4: 150 users concurrents and 20 users per second.\n",
    "- In the second stage were tested, the same tests, each one SCALED for 3\n",
    "\n",
    "<b><u>2. Tests results</b></u>\n",
    "\n",
    "![Test Locust 1](informe_locust1.jpg)\n",
    "![Test Locust 2](informe_locust2-02.jpg)\n",
    "\n",
    "<b><u>3. Conclusion</b></u>\n",
    "\n",
    "The best performance obteined without scaling the model was 4 requests per second with an error of 4.38%.\n",
    "\n",
    "Scaling the model for 3, the best performance was 5 requests per second with an error of 1.99%.  \n",
    "\n",
    "Therefore, the scalability of the model allows us to improve performance by 25% and reduce the error by 45%.  \n",
    "\n",
    "<b><u>4. Recomendations regarding optimization</b></u>\n",
    "\n",
    "As we could see, the scalability of the model generates a positive impact on the response of the application as the number of users increases. \n",
    "\n",
    "However, the decision to scale the model will depend on different variables, since the optimization of the models through scalability and improvement of the hardware in which the model works means an increase in costs. \n",
    "\n",
    "Therefore, the objectives of the business, the costs, the traffic that passes through the application, the expected traffic on exceptional dates and the staggered growth of the same, among other variables inherent to the application itself, must be analyzed. \n",
    "\n",
    "And based on all these factors, to be able to establish what would be the best way to scale the application and in what moments or time ranges. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
